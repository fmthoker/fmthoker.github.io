<!DOCTYPE html>
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Fida Mohammad Thoker- PhD student / Doktorand - University of Amsterdam</title>
    <link href="css/style.css" rel="stylesheet">
    <link href="css/header.css" rel="stylesheet">
    <link href="css/publications.css" rel="stylesheet">
    <link href="css/projects.css" rel="stylesheet">
    <link href="css/timeline.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Asap:400,400i,500" rel="stylesheet" type='text/css'>
    <style type="text/css"></style>

    <script type="text/javascript" src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.0/jquery.min.js"></script>
    <script type="text/javascript">
        $(document).ready(function(){
            $(".trigger").click(function(){ // trigger
                $(this).nextAll(".abstract").slideToggle("fast");
                $(this).children("a").toggleClass("closed open");
            });
        });
    </script>

</head>

<body>

<!-- HEADER -->
<div class="header">
    <div class="headermain">
        <div class="pubimg"><img src="img/fida.jpg" class="profilepic"></div>
        <div style="display: table-row">
            <div class="name">Fida Mohammad</div>
            <div class="affiliation">PhD Student at University of Amsterdam</div>
            <div class="affiliation">Video & Image Sense Lab  of Prof. dr. Cees Snoek</div>
            dd<div class="contact">fmthoker [at] gmail.com</div>
        </div>
    </div>
    <div class="menu">
        <table style="width: 100%;">
            <tr>
                <th class="menuentry" style="width: 25%;"><a href="mailto:fmthoker@gmail.com">fmthoker [at] gmail.com</a></th>
                <th class="menuentry" style="width: 50%;"><a href="https://scholar.google.com/citations?user=vqN8M3MAAAAJ&hl=en">Google Scholar</a></th>
                <th class="menuentry" style="width: 25%;"><a href=""https://www.linkedin.com/in/fmthoker/>Linkedin/Work</a></th>
            </tr>
        </table>
    </div>
</div>

<!-- CONTENT -->
<div class="main">

<!-- PUBLICATIONS -->
    <a name="publications">
    <div class="headline">Publications</div>
    <div class="publication">
        <div class="pubimg"><img src="publications/img/vssl.png" class="thumbnail"></div>
        <div style="display:table-row">
            <div class="pubtitle"> How Severe is Benchmark-Sensitivity in Video Self-Supervised Learning?  </div>
            <div class="pubauthors">Fida Mohammad Thoker, Hazel Doughty, Cees Snoek</div>
            <div class="pubinfo">To appear, European Conference on Computer Vision (ECCV), 2022</div>
            <!-- <div class="pubinfo">2021 Skeleton-Contrastive 3D Action Representation Learning, 2021</div>
PUBLICATIONS -->
            <div class="pubbox trigger"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/vssl.pdf">pdf</a></div>
            <div class="pubbox"><a href="">code</a></div>
            <div class="pubbox"><a href="publications/bibtex/vssl.bib">BibTex</a></div>
            <div class="abstract"> Despite the recent success of video self-supervised learning, there is much still to be understood about their generalization capability. In this paper, we investigate how sensitive video self-supervised learning is to the currently used benchmark convention and whether methods generalize beyond the canonical evaluation setting. We do this across four different factors of sensitivity: domain, samples, actions and task. Our comprehensive set of over 500 experiments, which encompasses 7 video datasets, 9 self-supervised methods and 6 video understanding tasks, reveals that current benchmarks in video self-supervised learning are not a good indicator of generalization along these sensitivity factors. Further, we find that self-supervised methods considerably lag behind vanilla supervised pre-training, especially when domain shift is large and the amount of available downstream samples are low. From our analysis we distill the SEVERE-benchmark, a subset of our experiments, and discuss its implication for evaluating the generalizability of representations obtained by existing and future self-supervised video learning methods.</div>
        </div>
    </div>
    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/skeleton_contrast.png" class="thumbnail"></div>
        <div style="display:table-row">
            <div class="pubtitle"> Skeleton-Contrastive 3D Action Representation Learning  </div>
            <div class="pubauthors">Fida Mohammad Thoker, Hazel Doughty, Cees Snoek</div>
            <div class="pubinfo">ACM International Conference on Multimedia (MM '21), 2021</div>
            <!-- <div class="pubinfo">2021 Skeleton-Contrastive 3D Action Representation Learning, 2021</div>
PUBLICATIONS -->
            <div class="pubbox trigger"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/skeleton-contrast.pdf">pdf</a></div>
            <div class="pubbox"><a href="https://github.com/fmthoker/skeleton-contrast">code</a></div>
            <div class="pubbox"><a href="publications/bibtex/skeleton-contrast.bib">BibTex</a></div>
            <div class="abstract"> This paper strives for self-supervised learning of a feature space suitable for skeleton-based action recognition.  Our proposal is built upon learning invariances to input skeleton representations and various skeleton augmentations via a noise contrastive estimation. In particular, we propose inter-skeleton contrastive learning, which learns from multiple different input skeleton representations in a cross-contrastive manner. In addition, we contribute several skeleton-specific spatial and temporal augmentations which further encourage the model to learn the spatio-temporal dynamics of skeleton data.  By learning similarities between different skeleton representations as well as augmented views of the same sequence,  the network is encouraged to learn higher-level semantics of the skeleton data than when only using the augmented views. Our approach achieves state-of-the-art performance for self-supervised learning from skeleton data on the challenging PKU and NTU datasets with multiple downstream tasks, including  action recognition, action retrieval and semi-supervised learning. Code is available at https://github.com/fmthoker/skeleton-contrast.</div>
        </div>
    </div>
    <!-- publication -->
    <div class="publication">
        <div class="pubimg"><img src="publications/img/feature-supervised.png" class="thumbnail"></div>
        <div style="display:table-row">
            <div class="pubtitle"> Feature-Supervised Action Modality Transfer  </div>
            <div class="pubauthors">Fida Mohammad Thoker, Cees Snoek</div>
            <div class="pubinfo">IEEE International Conference on Pattern Recognition (ICPR), 2020</div>
            <!-- <div class="pubinfo">2019 IEEE International Conference on Pattern Recognition (ICPR), 2020</div>
PUBLICATIONS -->
            <div class="pubbox trigger"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/feature-supervised.pdf">pdf</a></div>
            <div class="pubbox"><a href="publications/bibtex/feature-supervised.bib">BibTex</a></div>
            <div class="abstract"> This paper strives for action recognition and detection in video modalities like RGB, depth maps or 3D-skeleton sequences when only limited modality-specific labeled examples are available. For the RGB, and derived optical-flow, modality many large-scale labeled datasets have been made available. They have become the de facto pre-training choice when recognizing or detecting new actions from RGB datasets that have limited amounts of labeled examples available. Unfortunately, large-scale labeled action datasets for other modalities are unavailable for pre-training. In this paper, our goal is to recognize actions from limited examples in non-RGB video modalities, by learning from large-scale labeled RGB data.  To this end, we  propose a two-step training process: (i) we extract action representation knowledge from an RGB-trained teacher network and adapt it to a non-RGB student network. (ii) we then fine-tune the transfer model with available labeled examples of the target modality. For the knowledge transfer we introduce feature-supervision strategies, which rely on unlabeled pairs of two modalities (the RGB and the target modality) to transfer feature level representations from the teacher to the student network.  Ablations and generalizations with two RGB source datasets and two non-RGB target datasets demonstrate that an optical-flow teacher provides better action transfer features than RGB for both depth maps and 3D-skeletons, even when evaluated on a different target domain, or for a different task. Compared to alternative cross-modal action transfer methods we show a good improvement in performance especially when labeled non-RGB examples to learn from are scarce.</div>
        </div>
    </div>
    <div class="publication">
        <div class="pubimg"><img src="publications/img/cross-modal.png" class="thumbnail"></div>
        <div style="display:table-row">
            <div class="pubtitle">CROSS-MODAL KNOWLEDGE DISTILLATION FOR ACTION RECOGNITION</div>
            <div class="pubauthors">Fida Mohammad Thoker, Juergen Gall</div>
            <div class="pubinfo">IEEE International Conference on Image Processing (ICIP), 2019</div>
            <!-- <div class="pubinfo">2019 IEEE International Conference on Image Processing, ICIP 2019, Taipei, Taiwan, September 22-25, 2019.</div>
PUBLICATIONS -->
            <div class="pubbox trigger"><a href="javascript:void(0)">Abstract</a></div>
            <div class="pubbox"><a href="publications/pdf/cross-modal.pdf">pdf</a></div>
            <div class="pubbox"><a href="publications/bibtex/cross-modal.bib">BibTex</a></div>
            <div class="abstract"> In this work, we address the problem how a network for action recognition that has been trained on a modality like RGB videos can be adapted to recognize actions for another modality like sequences of 3D human poses. To this end, we extract the knowledge of the trained teacher network for the source modality and transfer it to a small ensemble of student networks for the target modality. For the cross-modal knowledge distillation, we do not require any annotated data. Instead we use pairs of sequences of both modalities as supervision, which are straightforward to acquire. In contrast to previous works for knowledge distillation that use a KL-loss, we show that the cross-entropy loss together with mutual learning of a small ensemble of student networks performs better. In fact, the proposed approach for cross-modal knowledge distillation nearly achieves the accuracy of a student network trained with full supervision.</div>
        </div>
    </div>



    <!-- EDUCATION -->
    <a name="education">
    <div class="headline">Education/Work</div>

    <!-- timeline -->
    <div class="timeline">
        <div class="entry">
            <div class="title">
                <p class="textbf">May 2019 - Present</p>
                <p>PhD Student, University of Amsterdam<p>
            </div>
        <div class="body">
            <p>Researcher in the field of Computer Vision.<br>Research focus on data efficient action recognition.</p>
        </div> 
        <!-- entry -->
        <div class="entry">
            <div class="title">
                <p class="textbf">Oct 2016 - Apr 2019</p>
                <p>Master's Degree, University of Bonn, Germany<p>
            </div>
        <div class="body">
            <p>Thesis: <i>Cross-modal Distillation for Action Recognition</i><br> A technique to transfer knowledge of action classification from one video modality to another.</i></p>
        </div>
        <!-- entry -->
        <div class="entry">
            <div class="title">
                <p class="textbf">Oct 2014 - May 2016</p>
                <p>Software Developer, Aricent Technologies, India<p>
            </div>
        <div class="body">
            <p>Project: <i>Optical Transport Networks</i><br> Development and maintainence of network protocols for optical transport network devices.</i></p>
        </div>
        <!-- entry -->
        <div class="entry">
            <div class="title">
                <p class="textbf">Jul 2010 - Jun 2014</p>
                <p>Bachelor's Degree, National Institute of Technology Srinagar, India<p>
            </div>
        <div class="body">
            <p>Project: <i>An android application for location based Augemented Reality.</i><br></p>
        </div>
    </div>


</div>

</body>
